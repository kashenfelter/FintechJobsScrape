{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BufferedWriter name='sample2.csv'>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "line=''\n",
    "s=set()\n",
    "flist=glob.glob(r'Doc3') #get all the files from the d`#open each file >> tokenize the content >> and store it in a set\n",
    "enc = 'utf-8'\n",
    "         \n",
    "tfile=open('filter.txt',\"r\", encoding = enc)\n",
    "line=tfile.read() # read the content of file and store in \"line\"\n",
    "tfile.close() # close the file\n",
    "s=s.union(set(line.split(' '))) # union of common words\n",
    "\n",
    "s=sorted(s) # sorts the content alphabetically\n",
    "\n",
    "\n",
    "i=0\n",
    "ct=0\n",
    "tf_line=''          \n",
    "doc_counts=[]       \n",
    "for term in s: #takes each term in the set \n",
    "    doc_counts.append(0)\n",
    "    \n",
    "        \n",
    "    doc=open('filter.txt',\"r\", encoding = enc)\n",
    "    line=doc.read()\n",
    "    doc.close()\n",
    "    ct=line.count(str(term)) #counts the no. of times \"term\" is present in the file\n",
    "    tf_line+=str(ct)+',' #prints the count in each doc seperated by comma\n",
    "    if (ct>0):              #counts no of docs in which \n",
    "        doc_counts[i]+=1    #this \"term\" is found\n",
    "    i+=1\n",
    "    tf_line=tf_line.strip()+'\\n'    \n",
    "\n",
    "idf=[]  #inverse document frequency      \n",
    "weights=[]      #weight\n",
    "total_docs=len(flist)   #total number of documents\n",
    "\n",
    "i=0\n",
    "\n",
    "for doc_count in doc_counts:    #takes the 1st doc count\n",
    "    idf.append(math.log(total_docs/doc_count)) #calculates idf for each \"term\"\n",
    "    weights.append(idf[i]*doc_count) #calculate weight of the term\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "final_line='TERM , D, IDF, TF-IDF\\n'       \n",
    "\n",
    "\n",
    "tf_arr=tf_line.split('\\n')\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "for term in s:\n",
    "    final_line+=term+','+tf_arr[i]+','+str(round(idf[i],5))+','+' '+str(round(weights[i],5))+','+'\\n'\n",
    "    i+=1\n",
    "\n",
    "fdoc=\"sample2.csv\"\n",
    "outfile=open(fdoc,\"wb\")\n",
    "outfile.write(final_line.encode())\n",
    "outfile.close()\n",
    "print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
