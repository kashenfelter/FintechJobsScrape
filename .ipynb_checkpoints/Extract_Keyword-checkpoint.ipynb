{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import csv\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import textrank\n",
    "\n",
    "english_stops = []\n",
    "\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    \"\"\" Lowercases, takes out punct and stopwords and short strings \"\"\"\n",
    "    return [token.lower() for token in tokens if (token not in string.punctuation) and\n",
    "               \t(token.lower() not in english_stops) and len(token) > 2]\n",
    "\n",
    "def get_stopwords():\n",
    "    enc = 'utf-8'\n",
    "    with open('stopword_file.csv', 'r', encoding = enc) as f:\n",
    "        reader = csv.reader(f)\n",
    "        keywords = list(reader)\n",
    "    english_stops = [i[0] for i in keywords]\n",
    "    #print ( english_stops)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "def get_cleanTokens(directory):\n",
    "    cleanTokens = []\n",
    "    wordcount = {} \n",
    "    for filename in os.listdir(directory):\n",
    "        text = get_text(\"KeywordDocs/\" + filename)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        cleanTokens.extend(clean_tokens(tokens))\n",
    "    return cleanTokens\n",
    "     \n",
    "def create_freqList(cleanTokens):\n",
    "    for tok in cleanTokens:\n",
    "        if tok not in wordcount:\n",
    "            wordcount[tok] = 1\n",
    "        else:\n",
    "            wordcount[tok] += 1\n",
    "        \n",
    "    sorted_wordCount = OrderedDict(sorted(wordcount.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    write_csv(sorted_wordCount)\n",
    "\n",
    "def write_csv(sorted_wordCount):\n",
    "    enc = 'utf-8'\n",
    "    if not os.path.isfile('frequency.csv'):\n",
    "        with open('frequency.csv', 'w', encoding = enc) as f:\n",
    "            for key in sorted_wordCount.keys():\n",
    "                f.write(\"%s,\\n\"%(key))\n",
    "    else:\n",
    "        with open('frequency.csv', 'a+', encoding = enc) as f:\n",
    "            for key in sorted_wordCount.keys():\n",
    "                f.write(\"%s,\\n\"%(key))\n",
    "\n",
    "\n",
    "def get_text(filename):\n",
    "    fp = open(filename, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def tf(word, blob):\n",
    "    return (float)(blob.words.count(word)) / (float)(len(blob.words))\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob)\n",
    "\n",
    "def idf(word, bloblist): return math.log(len(bloblist) / (float)(1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "\n",
    "def create_tfIdfList(document):\n",
    "    totalLength, splitLength = len(document), int(len(document)/400)\n",
    "    bloblist = [ document[i:i+splitLength] for i in range(0, totalLength, splitLength) ]\n",
    "    for i, blob in enumerate(bloblist):\n",
    "        blob = tb(blob)\n",
    "        scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        enc = 'utf-8'\n",
    "        with open('frequency.csv', 'a+', encoding = enc) as f:\n",
    "            columnTitleRow = \"Word, Score\\n\"\n",
    "            f.write(columnTitleRow)\n",
    "            for word, score in sorted_words: \n",
    "                score = \"{},{}\\n\".format(word, round(score, 5))\n",
    "                f.write(score)\n",
    "\n",
    "def create_textrankList(sample_text):\n",
    "    textrank_results = textrank.extractKeyphrases(sample_text)\n",
    "    for res in enumerate(textrank_results):\n",
    "        with open('textrank.csv', 'a+', encoding = 'utf-8') as f:\n",
    "            for word in res:\n",
    "                f.write(word + ', ')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_stopwords()\n",
    "    clean_tokens = get_cleanTokens(\"KeywordDocs\")\n",
    "    clean_doc = ' '.join(clean_tokens)\n",
    "    create_freqList(clean_tokens)\n",
    "    create_tfIdfList(' '.join(clean_tokens))\n",
    "    parts = [ clean_doc[i:i+int(len(clean_doc)/10)] for i in range(0, len(clean_doc), int(len(clean_doc)/10)) ]\n",
    "    for i,p in enumerate(parts):\n",
    "        create_textrankList(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/7d/ad09a26b63d4ad3f9395840c72c95f2fc9fa2b192094ef14e9e720be56f9/textblob-0.15.2-py2.py3-none-any.whl (636kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from textblob) (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\bhavy\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (3.4.0.3)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.2\n"
     ]
    }
   ],
   "source": [
    "! pip install textblob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
