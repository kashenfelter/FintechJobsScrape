{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-23187ca6dbc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Doc4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[0mtext_sents_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[0mdoc_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_sents_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[0mfreqDict_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_freq_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_sents_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0mTF_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeTF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfreqDict_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-23187ca6dbc2>\u001b[0m in \u001b[0;36mget_doc\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdoc_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_sents_clean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "\n",
    "text1=\"\"\"\"\n",
    "If you like pancakes combine it with maple syrup and whipped cream.\n",
    "It's really not that bad as it sounds.\n",
    "If the Easter bunny and the tooth fairy had babies would they take \n",
    "your teeth and laeve chocolate for you?\n",
    "\"\"\"\n",
    "\n",
    "def remove_string_special_character(s):\n",
    "    stripped = re.sub('[^\\w\\s]', '', s)\n",
    "    stripped = re.sub('-', '', stripped)\n",
    "    \n",
    "    stripped = re.sub('\\s+', '', stripped)\n",
    "    \n",
    "    stripped = stripped.strip()\n",
    "\n",
    "def get_doc(sent):\n",
    "    doc_info = []\n",
    "    i=0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1\n",
    "        count = count_words(sent)\n",
    "        temp = {'doc_id' : i,'doc_length' : count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info\n",
    "\n",
    "\n",
    "\n",
    "def count_words(sent):\n",
    "    count = 0\n",
    "    word = word_tokenize(sent)\n",
    "    for wor in words:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def create_freq_dict(sents):\n",
    "    i = 0\n",
    "    freqDict_list = [] \n",
    "    for sent in sents:\n",
    "        i +=  1\n",
    "        freq_dict = {}\n",
    "        word = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word] += 1\n",
    "            else:\n",
    "                freq_dict[word] = 1\n",
    "            temp = {'doc_id' : i,'freq_dict' : freq_dict}\n",
    "        freqDict_list.append(temp) \n",
    "    return freqDict_list   \n",
    "\n",
    "\n",
    "def computeTF(doc_info,dreqDict_list):\n",
    "    TF_scores = []\n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp = {'doc_id' : id,\n",
    "                   'TF_score' : tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'],'key' : k}\n",
    "            TF_scores.append(temp)   \n",
    "    return TF_scores       \n",
    "\n",
    "\n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id' : counter,'IDF_scores': math.log(len(doc_info)/count), 'key' : k}\n",
    "            \n",
    "            IDF_scores.appent(temp)\n",
    "            \n",
    "    return IDF_scores   \n",
    "\n",
    "\n",
    "\n",
    "def computeTFIDF(TF_scores,IDF_scores):\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id' : j['doc_id'],\n",
    "                       'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                       'key' : i['key']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores   \n",
    "\n",
    "\n",
    "file = open('Doc4','wb')\n",
    "text = file.read\n",
    "text_sents = sent_tokenize(text)\n",
    "text_sents_clean\n",
    "\n",
    "doc_info = get_doc(text_sents_clean)\n",
    "freqDict_list = create_freq_dict(text_sents_clean)\n",
    "TF_scores = computeTF(doc_info,freqDict_list)\n",
    "IDF_scores = computeIDF(doc_info,freqDict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
